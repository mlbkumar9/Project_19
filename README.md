
# Project 19: Automated Damage Segmentation

## 1. Project Overview

This project is designed to identify and quantify damage in images using computer vision and deep learning. It provides two main workflows:

1.  **Manual Scripts:** A set of individual scripts for step-by-step, manual execution of training and prediction for a single model at a time.
2.  **Automated Pipeline:** A fully automated, self-contained pipeline that trains and evaluates models for both Keras and PyTorch across a range of common backbones with a single command.

The core task is **image segmentation**, where the models are trained to identify the specific pixels that constitute damage.

## 2. Folder Structure

-   `/`: The root directory. Contains the manual scripts and original data folders.
-   `/RAW_Images/`: Contains the original, unprocessed images for training.
-   `/Masks/`: Contains the black-and-white mask files that label the exact location of damage. These are generated by `damage_analyzer.py`.
-   `/Input_Images_To_Analyze/`: A place to put new images for manual prediction.
-   `/Automated/`: A self-contained folder with the fully automated pipeline. It has its own internal copies of the data, models, and prediction outputs.
-   `/Trained_Models/`: Stores model files from the manual training scripts.
-   `/Predictions/`: Stores output images from the manual prediction scripts.

---

## 3. How to Use This Project

There are two ways to use this project. The automated pipeline is the recommended and most powerful approach.

### Workflow 1: Manual Operation (Step-by-Step)

Use the scripts in the main project folder for fine-grained control or debugging.

**➡️ Step 1: Generate Training Data**
First, you must generate the mask files that the neural networks will learn from.
-   **Run this command:** `python damage_analyzer.py`
-   **What it does:** It analyzes the images in `/RAW_Images/`, creates the corresponding mask files in `/Masks/`, and saves annotated images in `/Processed_Images/`.

**➡️ Step 2: Train a Single Model**
Edit one of the training scripts to choose a backbone, then run it.
-   **Edit:** Open either `train_keras_unet.py` or `train_pytorch_unet.py`.
-   **Change:** Modify the `BACKBONE` variable at the top of the file.
-   **Run:** Execute the script you just edited (e.g., `python train_pytorch_unet.py`).
-   **Result:** A new model file will be saved in `/Trained_Models/Keras/` or `/Trained_Models/Pytorch/`.

**➡️ Step 3: Predict with the Trained Model**
-   **Add Images:** Place one or more new images into the `/Input_Images_To_Analyze/` folder.
-   **Edit:** Open the prediction script that matches the framework you trained (e.g., `predict_keras.py`).
-   **Change:** Make sure the `BACKBONE` variable in this script **exactly matches** the one you used for training.
-   **Run:** Execute the prediction script.
-   **Result:** The output masks and labeled overlay images will be saved in the `/Predictions/` folder.

---

### Workflow 2: Fully Automated Pipeline (Recommended)

This is the easiest and most comprehensive way to use the project. The `/Automated/` folder contains everything needed to train and test both Keras and PyTorch models across all common backbones.

**➡️ How to Run:**
To execute the entire pipeline, simply run the master script from the project root directory:
-   **Run this command:** `python Automated/run_all.py`

**➡️ What It Does:**
The `run_all.py` script will automatically:
1.  Loop through a list of common backbones (ResNet, VGG, etc.).
2.  For each backbone, it will sequentially:
    -   Train a PyTorch U-Net model.
    -   Run prediction with the trained PyTorch model.
    -   Train a Keras U-Net++ model.
    -   Run prediction with the trained Keras model.
3.  It will print its progress to the console at every step.

**➡️ Where to Find the Results:**
All outputs from the automated pipeline are saved inside the `/Automated/` directory, keeping them separate from the manual runs:
-   **Models:** `Automated/Trained_Models/`
-   **Predictions:** `Automated/Predictions/`

The results are further organized into subdirectories by framework (Pytorch/Keras) and backbone name.

## 4. Other Scripts

-   `image_classifier_CV_ImageNet.py`: This is an auxiliary script. It does not perform damage segmentation. Instead, it uses a general-purpose ImageNet model to guess what object is in the image (e.g., "cassette", "shield"). Its results are saved in `imagenet_classification_results.csv`.

## 5. Dependencies

This project requires the following major Python libraries:
-   `tensorflow`
-   `torch`
-   `keras-unet-collection`
-   `segmentation-models-pytorch`
-   `opencv-python`
-   `scikit-learn`
